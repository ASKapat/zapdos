Really good articles on the moose-user list:

preconditioners (authored by Mariana Rodriguez)

Joule heating is working. It simply appears that currently the energy losses due to ionzation and elastic collisions outweigh the energy gains due to Joule heating.

Going to really start investigating what prevents taking long time steps. With no off diag jacobian elements for electrons and ions and off diag element for potential turned off, and with the following variable scalings:

em: 1e-11
Arp: 1e-6
potential: 1e4

the step size at 1e-7 seconds is 1.6e-9

em: 1e-6
Arp: 1e-6
potential: 1e4

the step size at 1e-7 seconds is 1.6e-9

Currently not able to even get to 1e-7 seconds with NEWTON. Perhaps my Jacobian is wrong. But it could also be right.

Best sim fail: somewhere in the microsecond range. Specs for this best simulation: nx = 4000, nl_rel_tol = 1e-2, hypre boomeramg, no scaling of any variables. Looks like fail time was: 3.0207e-6 seconds. I think a kink between .01 and .015 meters is what dooms the simulation. It's good to know that I can repeat this: after removing the stabilization which I implemented later in these notes, I reproduced this same later fail time, which is still the gold standard (assuming that that is a legitimate measure). 

These simulation failures are fundamentally different from the problems I was having before. Before the solutions were all smooth and pretty but the solves were either diverging or just going too slowly. I believe that this was simply because my relative tolerance was too high. If making the tolerance higher doesn't introduce oscillations in the solution and it allows the time steps to be much longer, than I am absolutely going to increase the tolerance! My priority is getting a solution. I don't care if there's too much diffusion for example. I don't give a damn about that. 

However, now I'm seeing oscillations. Fundamentally different problem. A better problem in my opinion. 

Three individual things I'm going to try (only change one variable at a time! Be patient!!!)

Test suite #1

1) Scale up the potential residual. Fail time: 5.50e-7 seconds
   Worse!
2) Increase the mesh density (Increased from 4000 to 8000 elements). Petsc failed with message: "Computed Nan differencing parameter h." Fail tiem again at 1.27e-8 seconds. So all the dependent variable solutions look beautifully smooth. However, there is an oscillation in the electron temperature at the cathode. This could be relevant because the electron temperature is in the source term for the electrons and ions. There are two things that should be done here. 1) I am going to try doubling the mesh resolution again to see if I can completely remove the oscillations. 2) It's worth looking into whether there are suggestions in combating the "Nan differencing parameter" for Petsc. Perhaps the recent messages on the Moose message board about Petsc error on fine mesh? 
a) From the fine mesh thread, which didn't make any reference to my particular error, a guy went from using ds for -mat_mffd_type to wp and his problem worked. -mat_mffd_type is for matrix free formulations (thus the mf). There is another petsc option, -mat_fd_type, which I believe is used if you're actually forming the Jacobian matrix (perhaps Newton?). 
b) In one thread from MOOSE mentioned the "Nan differencing parameter", John Mangeri said he was getting this error when using hypre and boomeramg. He said that when he went away from that preconditioner, he no longer experienced that problem. In a separate thread, Derek suggested that this generally means that a Nan is occurring somewhere in the residual, e.g. a divide by zero or sqrt of negative number. I personally have a hard time believing this though, especially considering my logarithmic formulation.

3) Decrease the relative tolerance. (Decreased from 1e-2 to 1e-3). Petsc failed with message: "Computed Nan differencing parameter h." Fail time: 1.27e-8 seconds. I didn't check how the solutions looked when the problem failed. If there were no oscillations in the solution, then the problem wasn't with the solution.
Way worse!

Test suite #2

1) Increased from 8000 to 16000 elements to see whether I can completely eliminate oscillations or see the "Nan differencing" error again. Fail time = 2.7911e-6. Reason for fail: looks like oscillations. It's always hard to know where the oscillations originate from. Is it oscillations in the bulk? Is it the oscillations at the boundary? Is it the electron temperature? Is it the electron density? Is it the argon density? The residuals of em, Arp, and mean_en are all about the same value. em is the highest. 

Oscillations are my problem. Paths to solution:

1) Stabilization -> Artificial diffusion
   Back to 4000 elements. With the stabilization, fail at 2.59839e-6 seconds. It looks like a kink forms around 5.5e-7 seconds. There aren't nearly as many oscillations in this simulation as in the 16000 element simulation. 
2) Mesh refinement -> Salome tutorial

Table of fail times for energy formulation:
4000 elements. nl_rel_tol = 1e-2. No stabilization. Fail time = 3.0207e-6 (normal fail)
4000 elements. nl_rel_tol = 1e-2. With stabilization. Fail time = 2.59839e-6 (normal fail)
4000 elements. nl_rel_tol = 1e-3. With stabilization. Fail time = 9.9839e-07 (normal fail)
16000 elements. nl_rel_tol = 1e-2. No stabilization. Fail time = 2.7911e-6 (normal fail)
8000 elements. nl_rel_tol = 1e-2. No stabilization. Fail time = 1.27e-8 (Nan differencing parameter)
4000 elements. nl_rel_tol = 1e-3. No stabilization. Fail time = 1.27e-8 (Nan differencing parameter)
8000 elements. nl_rel_tol = 1e-4. With stabilization. Fail time = 9.278e-08 (normal fail)
4000 elements. nl_rel_tol = 1e-2. No stabilization. Townsend form. Fail time = 6.493e-8 seconds. At least there no kinks this time. The problems were all in the oscillations at the cathode. 
4000 elements. nl_rel_tol = 1e-2. With stabilization. Townsend form. Fail time = 2.7639e-7 seconds. At least there no kinks this time. The problems were all in the oscillations at the cathode. And yes there are a lot of oscillations. Hahahahahahahah. Not really that funny. Guess what. I'm going to figure out how to mesh the hell out of that cathode region with salome. That's the next task! Hey at least stabilization helped that time!!! 

Note that the current Jacobian for the electrons for the Townsend formulation is absolutely wrong because I don't really know how to take the derivative of the absolute value of the flux. If the simulation fails because of this (pending that I can determine that that was indeed the cause of the failure), then I will invest some time into figuring out how to do that derivative correctly.

Running some simple tests, it looks like my scheme does indeed help stabilize. 

I went back to a local field formulation. And set a new world record for simulation time with (almost) all the physics included: Fail time = 1.92576e-5 seconds. Woooo! Hahaha. Going to add some stabilization for the ions because it looks like I'm getting some kinks and maybe some stabilization would help. I don't see any kinks in the electrons. 

Table of fail times for LFA formulation:
4000 elements. nl_rel_tol = 1e-2. Electron stabilization. Fail time = 1.92576e-5 (normal fail)
4000 elements. nl_rel_tol = 1e-2. Electron and ion stabilization. Fail time = 8.15811e-5 seconds (normal fail). New world record. Lots of oscillations in the electrons and ions, particularly the ions. This is likely without having a functional source term. But do those oscillations at really low concentration numbers matter? Because MOOSE is trying to minimize the residual, and in all places we are using the exponential of the concentration, or we are multiplying times the exponential of the concentration. 

I've added secondary electrons in and stuff is looking pretty damn good if I might say so myself. The problem is I got this error message after 3.8 microseconds (using ds for -mat_mffd_type):

[0]PETSC ERROR: Petsc has generated inconsistent data
[0]PETSC ERROR: Differencing parameter is not a number sum = nan dot = -nan norm = -nan
[0]PETSC ERROR: See http://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.

This error message occurred despite the fact that step sizes were still large, the previous time step's nonlinear solve had converged, and all the variable solutions were smooth. So if I can fix whathever problem caused the error message, I think the solver could keep going for quite a while. 

With -mat_mffd_type = wp, the simulation ran until 4.0 microseconds (so a little better). Solutions still looking smooth. This is the error message:

[0]PETSC ERROR: Petsc has generated inconsistent data
[0]PETSC ERROR: Computed Nan differencing parameter h
[0]PETSC ERROR: See http://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.

So slightly different error message. 

After scaling the potential by 1e13, the solve yielded the NaN differencing parameter at 3.071e-7 seconds (solve was proceeding well until that point.)

I changed the nl_rel_tol to 1e-3 (potential scaling still 1e13). With this setting, I incur a "normal" fail, e.g. no petsc errors. Solve failed at 3.3636e-6 seconds. Why did the solve fail? It's not because of the linear solve. In one of the later diverged non-linear solves, the linear residual is dropping just fine. However, the non-linear residual drops a couple orders of magnitude immediately but then refuses to drop any more. I don't know what the best strategy to approach this is. 

After scaling both the electrons and the ions by 1e2, the solve failed at 3.6097e-6 seconds, so a little later. Now when the solve is failing at the end of the simulation, the linear residuals are not dropping. Maybe this is a suggestion that I need to fix my preconditioner; well not fix, but add more terms and examine the petsc options for the preconditioner. 

I implemented all the Jacobian elements that I could think of and then switched to a Newton solve. The solve failed at 3.2548e-6 seconds. A normal fail with most of the crashes occurring because of diverged line search. Observe some oscillations where the electron temperature drops to zero in the mean_en plot. However, the mean_en is not the main contributor to the residual: the Arp kernel is. However, obviously em and Arp are both functions of mean_en and the electron temperature. However, the electron temperature actually looks smooth. The solve essentially fails as soon as the electron temperature touches zero. Is that a coincidence? I think not. 

Removing the ionization source term makes everything beautiful. The newton solve works beautifully...I get these nice giant implicit time steps. So it's the ionization source term that is gumming things up.

At the point where the mean_en goes to its small value, the value of mean_en - em is approximately -20...this will result in exp(-Eiz/(2/3*exp(mean_en - em))) having a value of 0, e.g. it's too small foating point value to be stored (I think). 

After going to 400 elements, the solve failed at 1.2209e-6 seconds. 

With the perfect preconditioner (finite difference preconditioner), the solve still failed. It failed at 5.8375e-6 seconds, so that is a world record. And what's interesting is that the electron temperature didn't touch zero as quickly. Two things left to try: 1) scale the potential residual. 2) Tighten the non-linear tolerance.

Scaled the potential residual: very interestingly the electron temperature touched down at zero earlier and the solve failed at 3.9943e-6 seconds as opposed to the much later time of 5.8375e-6 seconds without the potential scaling.

Last attempt: tighten the non-linear tolerance. After tightening the tolerances, the solve failed at 3.6947e-6 seconds. Again occurred immediately after touchdown of the electron temperature. I think it's safe to say that the problem is with my physics or with my residual implementation of the physics. I'm using a perfect preconditioner and a Newton solve, with smart choices for my relative and absolute tolerances. The problem is with the physics. 

I figured out why the source term wasn't having an effect on the solution: the pre-exponential factor simply wasn't large enough. If I increased it by two order of magnitude than the effects of the source were immediately apparent. Before making this change, I was using an alpha of 0.35 for air. How does that compare with number from bolos? It's incredibly small is what it is. For example, for an electric field of 1.5e7, alpha predicted by both bolos and bolsig is around 2e5. This is absurd compared to 0.35. 6 order of magnitude difference.

Yea, I solved this problem. Boom. Zapdos. Boom.

There's a lot of things to improve...for example Comsol took 10 minutes and 47 seconds to solve this problem on two processor. I believe that it took Zapdos 6 hours to solve on one processor. Sure, I could theoretically run Zapdos on many more processors, but then I need to fix my Jacobian. Comsol used a direct solver called MUMPS. I did not use a direct solver as far as I know. I would actually have to look at the default settings for Newton on one processor with a full preconditioning matrix formed from finite differencing of the residuals (I believe it's finite differencing of the residuals?). 

When I try to run with pc_type = lu, I get an error around the simulation time of 2.7e-5 seconds. The error is a petsc error and it says:
[0]PETSC ERROR: Zero pivot in LU factorization: http://www.mcs.anl.gov/petsc/documentation/faq.html#ZeroPivot
[0]PETSC ERROR: Zero pivot row 1 value 6.9212e-27 tolerance 2.22045e-14

I got this error twice. First time was with mat_fd_type = wp. Second time was with mat_fd_type = ds and with sub_pc_factor_shift = NONZERO. 

For p = 0, Discontinuous Galerkin is identical to first-order finite volume. That's really cool!

Trying the RF discharge. With default line search (I don't know what the default is, the moose list would lead me to believe that it's bt), the sim starts diverging at 3.63113e-8 seconds. With line_search = basic, the sim can't even take a single time step. Using line_search = cp, the sim starts diverging at t = 7.47679e-8 seconds. So I suppose that that's better. 

Reading the petsc manual, it looks like lu is the only direct solver available. To use it, use the options: -ksp_type=preonly; -pc_type=lu

From petsc manual: "Sometimes one is required to solve linear systems that are singular. That is systems with the matrix has a null space. For example, the discretization of the Laplacian operator with Neumann boundary conditions as a null space of the constant functions."

If trying to solve singular systems with direct solvers (or an incomplete factorization), it may still detect a zero pivot. Can run with additional options: -pc_factor_shift_type NONZERO -pc_factor_shift_amount <dampingfactor> to prevent the zero pivot. A good choice for the dampingfactor is 1.e-
10.

Files I need to fix after constructor update: RFIon, DCIon, RFElectron, DCElectron (both C and h files)

So for the RF plasma, keep in mind that I only calculated transport parameters for an electric field ranging from 1e3 to 1e6 V/m. There are problems with convergence for alpha with high electric fields (for this low pressure case of 1 torr). So after simulating, check and see whether the electric field stayed within those bounds. 
