Really good articles on the moose-user list:

preconditioners (authored by Mariana Rodriguez)

Joule heating is working. It simply appears that currently the energy losses due to ionzation and elastic collisions outweigh the energy gains due to Joule heating.

Going to really start investigating what prevents taking long time steps. With no off diag jacobian elements for electrons and ions and off diag element for potential turned off, and with the following variable scalings:

em: 1e-11
Arp: 1e-6
potential: 1e4

the step size at 1e-7 seconds is 1.6e-9

em: 1e-6
Arp: 1e-6
potential: 1e4

the step size at 1e-7 seconds is 1.6e-9

Currently not able to even get to 1e-7 seconds with NEWTON. Perhaps my Jacobian is wrong. But it could also be right.

Best sim fail: somewhere in the microsecond range. Specs for this best simulation: nx = 4000, nl_rel_tol = 1e-2, hypre boomeramg, no scaling of any variables. Looks like fail time was: 3.0207e-6 seconds. I think a kink between .01 and .015 meters is what dooms the simulation. It's good to know that I can repeat this: after removing the stabilization which I implemented later in these notes, I reproduced this same later fail time, which is still the gold standard (assuming that that is a legitimate measure). 

These simulation failures are fundamentally different from the problems I was having before. Before the solutions were all smooth and pretty but the solves were either diverging or just going too slowly. I believe that this was simply because my relative tolerance was too high. If making the tolerance higher doesn't introduce oscillations in the solution and it allows the time steps to be much longer, than I am absolutely going to increase the tolerance! My priority is getting a solution. I don't care if there's too much diffusion for example. I don't give a damn about that. 

However, now I'm seeing oscillations. Fundamentally different problem. A better problem in my opinion. 

Three individual things I'm going to try (only change one variable at a time! Be patient!!!)

Test suite #1

1) Scale up the potential residual. Fail time: 5.50e-7 seconds
   Worse!
2) Increase the mesh density (Increased from 4000 to 8000 elements). Petsc failed with message: "Computed Nan differencing parameter h." Fail tiem again at 1.27e-8 seconds. So all the dependent variable solutions look beautifully smooth. However, there is an oscillation in the electron temperature at the cathode. This could be relevant because the electron temperature is in the source term for the electrons and ions. There are two things that should be done here. 1) I am going to try doubling the mesh resolution again to see if I can completely remove the oscillations. 2) It's worth looking into whether there are suggestions in combating the "Nan differencing parameter" for Petsc. Perhaps the recent messages on the Moose message board about Petsc error on fine mesh? 
a) From the fine mesh thread, which didn't make any reference to my particular error, a guy went from using ds for -mat_mffd_type to wp and his problem worked. -mat_mffd_type is for matrix free formulations (thus the mf). There is another petsc option, -mat_fd_type, which I believe is used if you're actually forming the Jacobian matrix (perhaps Newton?). 
b) In one thread from MOOSE mentioned the "Nan differencing parameter", John Mangeri said he was getting this error when using hypre and boomeramg. He said that when he went away from that preconditioner, he no longer experienced that problem. In a separate thread, Derek suggested that this generally means that a Nan is occurring somewhere in the residual, e.g. a divide by zero or sqrt of negative number. I personally have a hard time believing this though, especially considering my logarithmic formulation.

3) Decrease the relative tolerance. (Decreased from 1e-2 to 1e-3). Petsc failed with message: "Computed Nan differencing parameter h." Fail time: 1.27e-8 seconds. I didn't check how the solutions looked when the problem failed. If there were no oscillations in the solution, then the problem wasn't with the solution.
Way worse!

Test suite #2

1) Increased from 8000 to 16000 elements to see whether I can completely eliminate oscillations or see the "Nan differencing" error again. Fail time = 2.7911e-6. Reason for fail: looks like oscillations. It's always hard to know where the oscillations originate from. Is it oscillations in the bulk? Is it the oscillations at the boundary? Is it the electron temperature? Is it the electron density? Is it the argon density? The residuals of em, Arp, and mean_en are all about the same value. em is the highest. 

Oscillations are my problem. Paths to solution:

1) Stabilization -> Artificial diffusion
   Back to 4000 elements. With the stabilization, fail at 2.59839e-6 seconds. It looks like a kink forms around 5.5e-7 seconds. There aren't nearly as many oscillations in this simulation as in the 16000 element simulation. 
2) Mesh refinement -> Salome tutorial

Table of fail times for energy formulation:
4000 elements. nl_rel_tol = 1e-2. No stabilization. Fail time = 3.0207e-6 (normal fail)
4000 elements. nl_rel_tol = 1e-2. With stabilization. Fail time = 2.59839e-6 (normal fail)
4000 elements. nl_rel_tol = 1e-3. With stabilization. Fail time = 9.9839e-07 (normal fail)
16000 elements. nl_rel_tol = 1e-2. No stabilization. Fail time = 2.7911e-6 (normal fail)
8000 elements. nl_rel_tol = 1e-2. No stabilization. Fail time = 1.27e-8 (Nan differencing parameter)
4000 elements. nl_rel_tol = 1e-3. No stabilization. Fail time = 1.27e-8 (Nan differencing parameter)
8000 elements. nl_rel_tol = 1e-4. With stabilization. Fail time = 9.278e-08 (normal fail)
4000 elements. nl_rel_tol = 1e-2. No stabilization. Townsend form. Fail time = 6.493e-8 seconds. At least there no kinks this time. The problems were all in the oscillations at the cathode. 
4000 elements. nl_rel_tol = 1e-2. With stabilization. Townsend form. Fail time = 2.7639e-7 seconds. At least there no kinks this time. The problems were all in the oscillations at the cathode. And yes there are a lot of oscillations. Hahahahahahahah. Not really that funny. Guess what. I'm going to figure out how to mesh the hell out of that cathode region with salome. That's the next task! Hey at least stabilization helped that time!!! 

Note that the current Jacobian for the electrons for the Townsend formulation is absolutely wrong because I don't really know how to take the derivative of the absolute value of the flux. If the simulation fails because of this (pending that I can determine that that was indeed the cause of the failure), then I will invest some time into figuring out how to do that derivative correctly.

Running some simple tests, it looks like my scheme does indeed help stabilize. 

I went back to a local field formulation. And set a new world record for simulation time with (almost) all the physics included: Fail time = 1.92576e-5 seconds. Woooo! Hahaha. Going to add some stabilization for the ions because it looks like I'm getting some kinks and maybe some stabilization would help. I don't see any kinks in the electrons. 

Table of fail times for LFA formulation:
4000 elements. nl_rel_tol = 1e-2. Electron stabilization. Fail time = 1.92576e-5 (normal fail)
4000 elements. nl_rel_tol = 1e-2. Electron and ion stabilization. Fail time = 8.15811e-5 seconds (normal fail). New world record. Lots of oscillations in the electrons and ions, particularly the ions. This is likely without having a functional source term. But do those oscillations at really low concentration numbers matter? Because MOOSE is trying to minimize the residual, and in all places we are using the exponential of the concentration, or we are multiplying times the exponential of the concentration. 

I've added secondary electrons in and stuff is looking pretty damn good if I might say so myself. The problem is I got this error message after 3.8 microseconds (using ds for -mat_mffd_type):

[0]PETSC ERROR: Petsc has generated inconsistent data
[0]PETSC ERROR: Differencing parameter is not a number sum = nan dot = -nan norm = -nan
[0]PETSC ERROR: See http://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.

This error message occurred despite the fact that step sizes were still large, the previous time step's nonlinear solve had converged, and all the variable solutions were smooth. So if I can fix whathever problem caused the error message, I think the solver could keep going for quite a while. 

With -mat_mffd_type = wp, the simulation ran until 4.0 microseconds (so a little better). Solutions still looking smooth. This is the error message:

[0]PETSC ERROR: Petsc has generated inconsistent data
[0]PETSC ERROR: Computed Nan differencing parameter h
[0]PETSC ERROR: See http://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.

So slightly different error message. 

After scaling the potential by 1e13, the solve yielded the NaN differencing parameter at 3.071e-7 seconds (solve was proceeding well until that point.)

I changed the nl_rel_tol to 1e-3 (potential scaling still 1e13). With this setting, I incur a "normal" fail, e.g. no petsc errors. Solve failed at 3.3636e-6 seconds. Why did the solve fail? It's not because of the linear solve. In one of the later diverged non-linear solves, the linear residual is dropping just fine. However, the non-linear residual drops a couple orders of magnitude immediately but then refuses to drop any more. I don't know what the best strategy to approach this is. 

After scaling both the electrons and the ions by 1e2, the solve failed at 3.6097e-6 seconds, so a little later. Now when the solve is failing at the end of the simulation, the linear residuals are not dropping. Maybe this is a suggestion that I need to fix my preconditioner; well not fix, but add more terms and examine the petsc options for the preconditioner. 

I implemented all the Jacobian elements that I could think of and then switched to a Newton solve. The solve failed at 3.2548e-6 seconds. A normal fail with most of the crashes occurring because of diverged line search. Observe some oscillations where the electron temperature drops to zero in the mean_en plot. However, the mean_en is not the main contributor to the residual: the Arp kernel is. However, obviously em and Arp are both functions of mean_en and the electron temperature. However, the electron temperature actually looks smooth. The solve essentially fails as soon as the electron temperature touches zero. Is that a coincidence? I think not. 

Removing the ionization source term makes everything beautiful. The newton solve works beautifully...I get these nice giant implicit time steps. So it's the ionization source term that is gumming things up.

At the point where the mean_en goes to its small value, the value of mean_en - em is approximately -20...this will result in exp(-Eiz/(2/3*exp(mean_en - em))) having a value of 0, e.g. it's too small foating point value to be stored (I think). 

After going to 400 elements, the solve failed at 1.2209e-6 seconds. 

With the perfect preconditioner (finite difference preconditioner), the solve still failed. It failed at 5.8375e-6 seconds, so that is a world record. And what's interesting is that the electron temperature didn't touch zero as quickly. Two things left to try: 1) scale the potential residual. 2) Tighten the non-linear tolerance.

Scaled the potential residual: very interestingly the electron temperature touched down at zero earlier and the solve failed at 3.9943e-6 seconds as opposed to the much later time of 5.8375e-6 seconds without the potential scaling.

Last attempt: tighten the non-linear tolerance. After tightening the tolerances, the solve failed at 3.6947e-6 seconds. Again occurred immediately after touchdown of the electron temperature. I think it's safe to say that the problem is with my physics or with my residual implementation of the physics. I'm using a perfect preconditioner and a Newton solve, with smart choices for my relative and absolute tolerances. The problem is with the physics. 

I figured out why the source term wasn't having an effect on the solution: the pre-exponential factor simply wasn't large enough. If I increased it by two order of magnitude than the effects of the source were immediately apparent. Before making this change, I was using an alpha of 0.35 for air. How does that compare with number from bolos? It's incredibly small is what it is. For example, for an electric field of 1.5e7, alpha predicted by both bolos and bolsig is around 2e5. This is absurd compared to 0.35. 6 order of magnitude difference.

Yea, I solved this problem. Boom. Zapdos. Boom.

There's a lot of things to improve...for example Comsol took 10 minutes and 47 seconds to solve this problem on two processor. I believe that it took Zapdos 6 hours to solve on one processor. Sure, I could theoretically run Zapdos on many more processors, but then I need to fix my Jacobian. Comsol used a direct solver called MUMPS. I did not use a direct solver as far as I know. I would actually have to look at the default settings for Newton on one processor with a full preconditioning matrix formed from finite differencing of the residuals (I believe it's finite differencing of the residuals?). 

When I try to run with pc_type = lu, I get an error around the simulation time of 2.7e-5 seconds. The error is a petsc error and it says:
[0]PETSC ERROR: Zero pivot in LU factorization: http://www.mcs.anl.gov/petsc/documentation/faq.html#ZeroPivot
[0]PETSC ERROR: Zero pivot row 1 value 6.9212e-27 tolerance 2.22045e-14

I got this error twice. First time was with mat_fd_type = wp. Second time was with mat_fd_type = ds and with sub_pc_factor_shift = NONZERO. 

For p = 0, Discontinuous Galerkin is identical to first-order finite volume. That's really cool!

Trying the RF discharge. With default line search (I don't know what the default is, the moose list would lead me to believe that it's bt), the sim starts diverging at 3.63113e-8 seconds. With line_search = basic, the sim can't even take a single time step. Using line_search = cp, the sim starts diverging at t = 7.47679e-8 seconds. So I suppose that that's better. 

Reading the petsc manual, it looks like lu is the only direct solver available. To use it, use the options: -ksp_type=preonly; -pc_type=lu

From petsc manual: "Sometimes one is required to solve linear systems that are singular. That is systems with the matrix has a null space. For example, the discretization of the Laplacian operator with Neumann boundary conditions as a null space of the constant functions."

If trying to solve singular systems with direct solvers (or an incomplete factorization), it may still detect a zero pivot. Can run with additional options: -pc_factor_shift_type NONZERO -pc_factor_shift_amount <dampingfactor> to prevent the zero pivot. A good choice for the dampingfactor is 1.e-
10.

Files I need to fix after constructor update: RFIon, DCIon, RFElectron, DCElectron (both C and h files)

So for the RF plasma, keep in mind that I only calculated transport parameters for an electric field ranging from 1e3 to 1e6 V/m. There are problems with convergence for alpha with high electric fields (for this low pressure case of 1 torr). So after simulating, check and see whether the electric field stayed within those bounds. 

Here is Comsol's electron and electron energy stabilization terms:

N_A_const*exp(-Ne*ccp.zeta)*test(Ne)
N_A_const*exp(-En*ccp.zeta)*test(En)

Simple and elegant. Only becomes large when Ne or En is small. Remember that Ne and En are the log of the actual density and total electron energy. This can be easily implemented when I return to an energy equation formulation. (Hell it could be implemented right now if I wanted to). Zeta is a parameter set by the user. Please start using user parameters again.

Having a lot of problems with diverged line search when the solution approaches steady state.

We want to solve F<mat>(x<vec>) = 0 where F is a nonlinear system of equations. (n equations, n unknowns in x<vec>)

Newton's method: x_k+1<vec> = x_k<vec> - J_inverse<mat>(x_k<vec>) * F<mat>(x_k<vec>)

J = F_prime, e.g. the Jacobian is the derivative of F with respect to the system variables (x)

In order to use this method, the Jacobian must be invertible, e.g. it cannot be singular. Singular matrices cannot be inverted. 

Analytic expressions for the Jacobian can be provided by the user, or they can be calculated somewhat automatically (and approximately) by finite differencing F (the residuals). 

For a Newton solve in which the Jacobian elements are provided by the user, the -mat_fd_type makes no sense. The only time -mat_fd_type makes sense is when the Jacobian is calculated with finite differencing. One can also use a matrix free formulation, and then the option is -mat_mffd_type. 

To my mind the Jacobian and preconditioning matrices are two different matrices. But I'm not sure. 

Pre-conditioning is done in iterative linear solves. LU is a direct solver; it's a full inversion of the Jacobian matrix. Thus, if we're using LU, there is no preconditioning. 

From Wikipedia for solving the system:

A*x = b, we instead solve A*P_inverse*y = b, where y = P*x (right preconditioning) or P_inverse*(A*x-b) = 0 (left preconditioning, apparently more common). 

"Typically there is a trade-off in the choice of P. Since the operator P^{-1} must be applied at each step of the iterative linear solver, it should have a small cost (computing time) of applying the P^{-1} operation. The cheapest preconditioner would therefore be P=I since then P^{-1}=I. Clearly, this results in the original linear system and the preconditioner does nothing. At the other extreme, the choice P=A gives P^{-1}A = AP^{-1} = I, which has optimal condition number of 1, requiring a single iteration for convergence; however in this case P^{-1}=A^{-1}, and applying the preconditioner is as difficult as solving the original system. One therefore chooses P as somewhere between these two extremes, in an attempt to achieve a minimal number of linear iterations while keeping the operator P^{-1} as simple as possible. Some examples of typical preconditioning approaches are detailed below."

Thus to give the minimal number of linear iterations, P should equal A. (However, minimal number of linear iterations probably is not your only criteria for an efficient, accurate simulation). So A, wikipedia's notation, is actually the Jacobian in my problems. Thus the perfect preconditioner (in terms of least numer of linear iterations) for the linear solve, would actually be the Jacobian itself!; using the Jacobian itself would result in only one linear iteration. 

-snes_mf_operator: Activates default matrix-free Jacobian-vector products, and a user-provided preconditioning matrix as set by SNESSetJacobian() 

MOOSE delineates two things: the preconditioning matrix, and the preconditioning process. 

Oh, very interesting. Looking at the doco for SNESSetJacobian, there are two input matrices: A, the Jacobian matrix, and B, the preconditioner matrix, which "is usually the same as the Jacobian."

Thus Jed Brown's suggestion of using: "-snes_mf_operator -pc_type lu to see if the Jacobian you are
using is wrong." still doesn't make total sense to me. Ok, I think all of it is mostly making sense now. -snes_mf_operator activates default matrix-free Jacobian-vector products, and a user-provided preconditioning matrix as set by SNESSetJacobian(). As soon as we move to a matrix-free formulation, I believe we have to be using iterative linear solves. And that means using a preconditioner. How do we create the preconditioning matrix? By default it just uses the diagonals, e.g. whatever diagonal jacobian elements we've coded. Using SMP, full=true, we can say to build the entire preconditioning matrix, setting it equal (hopefully) to the Jacobian. However, we can also create the preconditioning matrix using finite differencing of the residual statements in the hope of creating a good approximation of the jacobian.

My hypothesis if I turn off the FDP preconditioner and use Jed's -snes_mf_operator -pc_type lu is that my preconditioning matrix will be formed from my diagonal jacobians (which are not well coded) and then I will be doing a "direct" matrix-free solve using lu. So I think that's what I saw; I saw some linear solves that were quite terrible. I hypothesize that that's because my preconditioning matrix was terrible. However, something that was quite interesting, and that makes me kind of happy, is that with Jed's options, it only took one non-linear iteration to solve the first time step. I'm not sure that I've seen that before. The solve failed at 2.94834e-6 seconds. Ok, now I'm going to try with Jed's options but with my preconditiong matrix set equal to the finite differenced jacobian. I'm hoping that this eliminates any bad linear solves. That's my hypothesis at least. 

When doing direct matrix solves, the Jacobian affects the nonlinear solution strategy. However, in matrix free solves, the Jacobian is not actually present. It's only loosely present in that you use some emulation of the Jacobian in constructing your preconditioner that's used in the linear iterative solve. So in a Newton matrix solve, the effect of the Jacobian will be pronounced in the non-linear steps. In a New matrix free solve, the effect of the Jacobian will be pronounced in the linear steps. So I could play around with -mat_mffd_type

I believe that in any matrix free formulation is going to involve finite differences and a differencing parameter. E.g. the action of the Jacobian on a vector is approximated using finite differences! So I believe I actually have two finite differencing operations coming into play: 1) I am constructing my Jacobian/Preconditionig matrix using finite differencing of my residuals. 2) Then I am approximating the action of my Jacoban/Preconditioning matrix on a vector using finite differences. #2 comes into play when I decide to go to a matrix free formulation. It's possible to have no finite differencing in your problem: 1) construct your jacobian using analytical expressions 2) don't use a matrix free formulation. Simulation using the FDP preconditioner seems quite stable. It would probably go all the way to completion if I let it run. First divergence occurred at 1.14239e-5 seconds. Going to go back to direct LU and see when first divergence occurs. With direct LU, first divergence occurs at 1.37097e-5 seconds, so a little better.

The default KSP type is GMRES with a restart of 30, using modified Gram-Schmidt orthogonalization. 

With default differencing parameter (I believe wp), here's Jacobian results:

Norm of matrix ratio 4.08771e-07 difference 0.0207982 (user-defined state)
Norm of matrix ratio 6.25133e-09 difference 8.56263e+09 (constant state -1.0)
Norm of matrix ratio 5.08948e-09 difference 9.62509e+08 (constant state 1.0)

With mat_fd_type=ds, Jacobian results:

Norm of matrix ratio 0.00932033 difference 474.28 (user-defined state)
Norm of matrix ratio 6.4317e-29 difference 8.80968e-11 (constant state -1.0)
Norm of matrix ratio 3.21208e-27 difference 6.0746e-10 (constant state 1.0)

With mat_fd_type=ds, mat_fd_coloring_err=1e-6, Jacobian results:

Norm of matrix ratio 7.87438e-05 difference 4.00641 (user-defined state)
Norm of matrix ratio 4.66307e-07 difference 6.38715e+11 (constant state -1.0)
Norm of matrix ratio 5.01161e-07 difference 9.47782e+10 (constant state 1.0)

I'm curious what the default mat_fd_coloring_err is, or if there is a default So if low numbers are generally better, maybe I should try ds? The two constant states are amazing but the user-defined state doesn't seem to be as good as wp. Again, I don't know what that means. -snes_type = test, can only be run if solve_type is set to NEWTON. It cannot be used JFNK. (matrix free methods). 

With ds, the solve first diverged at 1.37097e-5 seconds, so it appears to have been identical to wp. With mat_fd_coloring_err=1e-6, the solve crashed almost immediately. So that pretty well reflected the Jacobian results!

In the IntTD.C materials file, I took away eta, so all input files need to be free of an eta column.

So I've coded the Jacobians for the kernels; I forgot about the bloody boundary conditions!!!

A huge damn victory for Moose and Zapdos tonight!!!!!! Using my analytical jacobian, a Newton solve, a direct LU method, I solved the problem to steady state in 168 seconds :-) Take that comsol!!!!

Testing Jacobian after adding back in the Electron Energy equation. Hand-coded Jacobian:

Norm of matrix ratio 0.00756585 difference 384.97 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

FDP Jacobian:

Norm of matrix ratio 3.78445e-06 difference 0.192568 (user-defined state)
Norm of matrix ratio 1.21061e-07 difference 1.65821e+11 (constant state -1.0)
Norm of matrix ratio 1.22993e-07 difference 2.326e+10 (constant state 1.0)

Without BCs:

Norm of matrix ratio 0.00756585 difference 384.97 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

Without electron energy time derivative:

Norm of matrix ratio 0.00756608 difference 384.97 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

Without electron energy kernel:

Norm of matrix ratio 3.96561e-06 difference 0.201765 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

That last test kind of reveals where the error is, doesn't it?

After maybe modifying the source terms in the ElectronEnergyKernel:

Norm of matrix ratio 0.00756585 difference 384.97 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

After removing Joule heating:

Norm of matrix ratio 0.00756585 difference 384.97 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

After removing all but advection and diffusion:

Norm of matrix ratio 9.24413e-06 difference 0.470329 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

Same but using FDP:

Norm of matrix ratio 3.78303e-06 difference 0.192476 (user-defined state)
Norm of matrix ratio 1.21061e-07 difference 1.65821e+11 (constant state -1.0)
Norm of matrix ratio 1.22993e-07 difference 2.326e+10 (constant state 1.0)

With all physics in ElectronEnergyKernel removed:

Norm of matrix ratio 3.96561e-06 difference 0.201765 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

Two things to do:
1) Make a couple of results summary slides of what I've done so far in Zapdos
2) 5 modules (kernels, BCs) I would want to add to make Zapdos/Moose relevant for broader LTP community

The steady state electron density as determined by zapdos does not differ based on whether an iterative or direct solve is used. The peak electron density for zapdos (both direct and iterative solutions) and the peak electron density for comsol differ by 5.1% (Which is actually fairly significant I suppose). 

MKL PARDISO: Intel Math Kernel Library PARallel Direct SOlver

PARDISO in Intel MKL was originally developed by the Department of Computer Scienc at the University of Basel. It can be obtained at http://www.pardiso-project.org

After adding back in advection and diffusion and on-diagonal jacobians. I've reviewed these residuals and jacobians and they are exactly correct.

Norm of matrix ratio 7.10444e-06 difference 0.361464 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

After adding back advection off-diag jacobian. After review, this looks perfect.

Norm of matrix ratio 9.24413e-06 difference 0.470329 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

After adding in Joule-heating and associated potential off-diag jacobian:

Norm of matrix ratio 9.24413e-06 difference 0.470329 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

After adding in Joule-heating and associated em off-diag jacobian:

Norm of matrix ratio 9.24413e-06 difference 0.470329 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

After adding back reaction source and on-diag jacobian:

Norm of matrix ratio 0.00756587 difference 384.97 (user-defined state)
Norm of matrix ratio 1.57362e-07 difference 2.15543e+11 (constant state -1.0)
Norm of matrix ratio 1.56667e-07 difference 2.96284e+10 (constant state 1.0)

When trying to solve using super lu, I keep getting diverged_line_search after 0 iterations when I use the default line search. If I set line_search='none', then the initial divergence occurs at (at least approximately) the same time step, but the divergence message is: diverged_fnorm_nan. 

When solving in parallel with moose's default parallel iterative options, the solve fails at 1.69359e-7 seconds. The final divergence reason is given by:

  Linear solve did not converge due to DIVERGED_NANORINF iterations 0
Nonlinear solve did not converge due to DIVERGED_LINEAR_SOLVE iterations 2

Looking at all the divergences, the DIVERGED_LINEAR_SOLVE was about as frequent as DIVERGED_LINE_SEARCH.

Just confirming that in this test problem the Jacobian is good, when I run the fast_solve_parallel_iterative_attempt in test mode, with a mesh size of 1, here's what the results look like:

Norm of matrix ratio 4.44187e-08 difference 0.00226001 (user-defined state)
Norm of matrix ratio 2.33137e-08 difference 3.19335e+10 (constant state -1.0)
Norm of matrix ratio 2.20073e-08 difference 4.16197e+09 (constant state 1.0)

So excellent. When I change the mesh size to 100, here are the results (still good):

Norm of matrix ratio 1.58941e-07 difference 0.00808669 (user-defined state)
Norm of matrix ratio 3.48091e-08 difference 4.7679e+10 (constant state -1.0)
Norm of matrix ratio 3.26732e-08 difference 6.17907e+09 (constant state 1.0)

What am I trying to do right now? I'm trying to get my gold_serial_case to solve in parallel mode, using 1) iterative methods and 2) direct methods. Another thing that I've been working on is adding back an electron energy equation; I'm currently debugging the Jacobian for that.

I'm looking at the iterative solve right now. If I run with one mesh element, I get a floating point exception which appears to occur at line 52 in ElectronKernel.C. However, if I run with 100 mesh elements, than I no longer get any FPEs/program breaks when running ./zapdos-dbg (without gdb --args). So the FPE does indeed come from an exponential overflow. The last value of _u[_qp] before the floating point exception is signalled is: 7.88655e+07. Since I know that exp(1000) produces inf, then obviously 7.89e7 will also produce an inf. This exercise kind of just goes to show that you need an appropriate number of mesh elements or else your solution will have major problems.

I ran the direct solve with one mesh element (./zapdos-dbg) and got no FPEs.

Don't forget how to print values in the debugger:

p <variable>

or 

print <variable>

Ok, using default MOOSE iterative with mpirun and an abs tolerance of 1e-6 (e.g. all the parameters the same from the fast serial lu gold solve except for the solve type), the solve fails at 2.49633e-07 seconds. All the divergence reasons are: DIVERGED_LINE_SEARCH. First divergence occurs at: 4.84966e-08 seconds. No line_search option specified for the settings in this passage (so default). Now the iterative solve has major problems with its solution variables, and this could easily explain why this solve fails. There are clear spikes at L/4, L/2, and 3L/4 in the dependent variable Arp and in the auxiliary variable EField. So again, parallel iterative solves are failing. 

Using direct super LU with line_search='none', first divergence is at 1.28117e-07 seconds, and the message is DIVERGED_FNORM_NAN. All the divergence messages are DIVERGED_FNORM_NAN. Final failure occurs at 2.40013e-07 seconds. (So fairly comparable to the iterative attempt). With default line_serach, first divergence is identically at 1.28117e-07 seconds. All divergence reasons are DIVERGED_LINE_SEARCH. Final failure identically occurs at 2.40013e-07 seconds. So my initial intuitive reaction was correct: despite the change in line_search, the solve patterns are the exact same. Looking at the default line search results in paraview, every solution variable looks just fine at the premature end of the simulation: all variable profiles are smooth and continuous, so any reason for the solve fail can't really be found there.

The fun part now in both direct and iterative cases is to figure out: what's causing the diverged line search???

Alright, doing some cool stuff. Did a serial iterative solve. | Zapdos Performance: Alive time=150.776, Active time=149.795. pc_type=ilu, ksp_type=gmres, snes_type=newtonls. Results look perfect. 

With pc_type=gamg, the problem wasn't even able to take a single time step.

hypre is meant for massively parallel computing (taken from its homepage). 

With pc_type=asm, sub_pc_type=ilu, the problem solves. Solution summary: | Zapdos Performance: Alive time=159.017, Active time=158.009. 

With:

  petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type -sub_pc_factor_shift_amount'
  petsc_options_value = 'asm lu NONZERO 1.e-10'

Solution summary: | Zapdos Performance: Alive time=153.569, Active time=152.56

With pc_type=bjacobi, the problem solves. Solution summary: | Zapdos Performance: Alive time=149.462, Active time=148.471 (Record)

Next step: try suggestions from petsc email for direct solves.

This should be my compiler environment the next time I update_and_rebuild libmesh and recompile MOOSE and Zapdos:
export CC="ccache mpicc"
export CXX="ccache mpicxx"
export CCACHE_SLOPPINESS=time_macros

Oh heck yea: solved my problem in parallel!!!!!!!!! Wooooooooooooooooooooooo!!!!!!!!!!!!!!!!! Got mumps working correctly and it solved like a bloody damn champion. To summarize here were the petsc options:

  petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_shift_amount -ksp_type -pc_factor_mat_solver_package'
  petsc_options_value = 'lu NONZERO 1.e-10 preonly mumps'

And here is the execution summary: | Zapdos Performance: Alive time=133.099, Active time=131.212

So having the parallel solve didn't really speed up the solution, but it's a demonstration of principle gosh darn it!!!

Ok, this is really freaking weird. I went back to an old commit where there was no ZapdosRevision.h file, but presumably there should have been no new source code changes to compile, and when I typed make -j4, here's the first output: 

MOOSE Generating Header /home/lindsayad/projects/zapdos/include/ZapdosRevision.h...

Then it proceeds to compile zapdos source files (presumably it was going to compile all of them before I manually stopped it.)

However, when I reset my pointer to point at the newest commit (where ZapdosRevision.h is included), it still recompiled all my source code, only this time it didn't have that first line: 

MOOSE Generating Header /home/lindsayad/projects/zapdos/include/ZapdosRevision.h...

It's important to note that after recompilation, my gold mumps script executed without complaint, so that helps to assuage some of my fears that recompilation would prevent me from seeing petsc packages.

Using parallel Bjacobi with lu as the sub_pc_type, the solve failed at 2.35183e-07 seconds. And sure enough, I see the familiar kinks in the solution in the places where the processors come together. This was done on 4 processors.

However, when I went to 2 processors the problem solved! Performance summary: | Zapdos Performance: Alive time=111.615, Active time=110.712 (Record!)

In early time steps, I could still see some kinks in the Arp solution and occasionally even in the em solution. However, at longer time steps the kinks disappeared. I would hypothesize that these kinks only appear when the potential is almost perfectly linear and the electric field is incredibly uniform such that any tiny miscommunication between the processors will induce a very noted change in the electrostatics on an automatic scale. And these kinks in the electrostatics directly impact the particle drift motion. However, on longer time scales when the particles begin to affect the electric field, these affects are much stronger and on a much larger scale than the processor miscommunication so that the processor miscommunication no longer becomes a problem.

With that same run as the one immediately above on the desktop, the problem solved in 97 seconds with console output to the screen. When I tried running with 4 processors, I got the same crash that I've come to know. It also occurred at 2.35183e-07 seconds. So the success of running on 2 cores comes not from having the correct number of physical cores but from the formulation of the problem.

Reducing the amount of output to the log file doesn't seem to have helped the problem solve any faster. Scaling comparison:

2 processors: 97.0632 seconds
1 processor: 141.503 seconds

So computation time was reduced by 31%. E.g. it took 69% as long with 2 processors as it did with 1. With perfect parallelization that 69 number would be 50.

Here's the Jacobian test output for this ionization source residual and corresponding Jacobian: 

    -_test[_i][_qp]*_rate_coeff_ion_en[_qp]*_Ar[_qp]*-_Eiz_en[_qp]*_em[_qp]

Norm of matrix ratio 7.86836e-08 difference 0.00400463 (user-defined state)
Norm of matrix ratio 1.76423e-08 difference 2.41652e+10 (constant state -1.0)
Norm of matrix ratio 1.9803e-08 difference 3.74509e+09 (constant state 1.0)

Here's the Jacobian test output for this ionization source residual and corresponding Jacobian: 

    -_test[_i][_qp]*_rate_coeff_ion_en[_qp]*_Ar[_qp]*-_Eiz_en[_qp]*_em[_qp]*em[_qp]

Norm of matrix ratio 7.86836e-08 difference 0.00400463 (user-defined state)
Norm of matrix ratio 1.76423e-08 difference 2.41652e+10 (constant state -1.0)
Norm of matrix ratio 1.9803e-08 difference 3.74509e+09 (constant state 1.0)

Here's the Jacobian test output for this ionization source residual and corresponding Jacobian: 

    -_test[_i][_qp]*_rate_coeff_ion_en[_qp]*_Ar[_qp]*-_Eiz_en[_qp]*std::sqrt(_em[_qp])

Norm of matrix ratio 7.86834e-08 difference 0.00400462 (user-defined state)
Norm of matrix ratio -nan difference -nan (constant state -1.0)
Norm of matrix ratio 1.9803e-08 difference 3.74509e+09 (constant state 1.0)

Here's the Jacobian test output for this ionization source residual and corresponding Jacobian: 

    -_test[_i][_qp]*_rate_coeff_ion_en[_qp]*_Ar[_qp]*-_Eiz_en[_qp]*std::exp(_em[_qp])

Norm of matrix ratio 1.45574 difference 74090.6 (user-defined state)
Norm of matrix ratio 1.76423e-08 difference 2.41652e+10 (constant state -1.0)
Norm of matrix ratio 1.9803e-08 difference 3.74509e+09 (constant state 1.0)

Here's the Jacobian test output for this ionization source residual and corresponding Jacobian: 

    -_test[_i][_qp]*_rate_coeff_ion_en[_qp]*_Ar[_qp]*-_Eiz_en[_qp]*std::exp(_u[_qp])

Norm of matrix ratio 4.08685e-08 difference 0.0123279 (user-defined state)
Norm of matrix ratio 1.76423e-08 difference 2.41652e+10 (constant state -1.0)
Norm of matrix ratio 1.9803e-08 difference 3.74509e+09 (constant state 1.0)

I think I'm figuring out what the constant states mean in the Jacobian test. I believe that constant state 1.0 is where all the dependent variables are set to 1.0 (an IC). User-defined state would be with my defined ICs. Yes my hypothesis is confirmed. So if I set all my variables to ICs of 1 or -1, my Jacobian looks really damn good!

It should also be noted that the variable scaling affects the "Norm of matrix ratios."

Trying with electron energy formulation again. Solve failed at 1.05637-07 seconds. All the variables look pretty nice and smooth, so I don't really know what the problem is. Divergence reasons were fairly evenly split between diverged_max_its and diverged_line_search. At the last convergence step, here were the variable residual norms:

                  potential: 4.042e-10
                  em:        2.6627e-08
                  Arp:       3.20928e-10
                  mean_en:   0.000475877

With an abs_tolerance=1e-2 and using mumps in parallel, the solve still failed at 1.05637e-07 seconds. With the ionization source terms elimination in the electron energy equation, the solve actually failed earlier at 7.59991e-08 seconds. This actually makes since because I still have physical production of electrons and ions in their respective kernels. Dumb me.

Tried again with electron energy formulation, but using PJFNK instead of NEWTON. Solve got a bit farther this time: failed at 3.39951e-07 seconds. Instead of any DIVERGED_LINE_SEARCH, all fails were due to DIVERGED_MAX_IT. i think diverged_line_search only really appears when doing Newton solves. The only reason it went farther is because I forgot to turn the ionization terms in the em and ion kernels back on again. With those turned back on, the solve fails at 1.08282e-07 seconds. Again, looking at the solution variables, I can see no reason why the solve should fail. However, if I look at the log, the linear residuals don't seem to be coming down nearly fast enough, suggesting to my continued bewilderment, that my Jacobian is somehow wrong. I'm going to try to solve using FDP.

Using FDP and a direct LU solve, the solve failes at 1.113e-07 seconds. Record, wooo! Haha...no. Why the hell does it fail? All the variables are gosh darn smooth! I don't see any oscillations anywhere!

With block-jacobi:

 9 Nonlinear |R| = 5.601308e-06
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 9
 Solve Converged!

With asm:

 9 Nonlinear |R| = 5.601308e-06
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 9
 Solve Converged!

So I have no bloody idea whether my command line petsc options are working. Actually it most certainly appears to not be working because when I pass -ksp_type=preonly on the command line, linear iterations still appear in the log file. Petsc options are still working from the input file, however. Another thing that appears to not be working is that when I pass the -help option on the command line, the values that appear in <> for the petsc variables don't represent the values being used in my program. Rather they appear to represent the defaults. This is strange because I feel like I have a memory of being able to see my values represented in <>. 

I see the same behavior on my laptop. Perhaps it's a petsc version thing. Perhaps with version 3.5.x of petsc I could see the values of the variables I had changed instead of the defaults. It would be quite nice to confirm that I've changed the variables that I think I've changed.
